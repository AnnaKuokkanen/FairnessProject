{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3\n",
    "\n",
    "Dataset: COMPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7214, 53)\n",
      "Index(['id', 'name', 'first', 'last', 'compas_screening_date', 'sex', 'dob',\n",
      "       'age', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
      "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
      "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
      "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
      "       'c_charge_degree', 'c_charge_desc', 'is_recid', 'r_case_number',\n",
      "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
      "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
      "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
      "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
      "       'decile_score.1', 'score_text', 'screening_date',\n",
      "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
      "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
      "       'start', 'end', 'event', 'two_year_recid'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"compas-scores-two-years.csv\")\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first fit a model predicting if the person will reoffend in the next two years. We could use a classifier for this. \n",
    "\n",
    "Next steps:\n",
    "\n",
    "* look at the data for bias\n",
    "* build a classifier\n",
    "\n",
    "Sensitive variables: race, sex \n",
    "\n",
    "Target variable: two_year_recid\n",
    "\n",
    "Features: age/age_cat, juv_fel_count, juv_misd_count, juv_other_count, priors_count, c_charge_degree, score_text, days_b_screening_arrest, decile_score, length_of_stay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twin test \n",
    "\n",
    "Feature: prior convictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annku\\AppData\\Local\\Temp\\ipykernel_10092\\3782434972.py:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"age_cat\"] = df[\"age_cat\"].replace({\"Less than 25\": 0, \"25 - 45\": 1, \"Greater than 45\": 2})\n"
     ]
    }
   ],
   "source": [
    "df = df[[\"race\", \"sex\", \"age_cat\", \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\", \"priors_count\", \"c_charge_degree\", \"two_year_recid\"]]\n",
    "\n",
    "df = df[(df[\"c_charge_degree\"] != 'O')]\n",
    "\n",
    "# sum 1-20 together\n",
    "df[\"juv_fel_count\"] = np.where(df[\"juv_fel_count\"] == 0, 0, 1)\n",
    "df[\"juv_misd_count\"] = np.where(df[\"juv_misd_count\"] == 0, 0, 1)\n",
    "df[\"juv_other_count\"] = np.where(df[\"juv_other_count\"] == 0, 0, 1)\n",
    "df[\"priors_count\"]\n",
    "\n",
    "df.loc[(df['priors_count'] >= 0) & (df['priors_count'] <= 10), 'priors_count'] = 0\n",
    "df.loc[(df['priors_count'] > 10) & (df['priors_count'] <= 20), 'priors_count'] = 1\n",
    "df.loc[(df['priors_count'] > 20), 'priors_count'] = 2\n",
    "\n",
    "df[\"c_charge_degree\"] = np.where(df[\"c_charge_degree\"] == \"M\", 0, 1)\n",
    "df[\"age_cat\"] = df[\"age_cat\"].replace({\"Less than 25\": 0, \"25 - 45\": 1, \"Greater than 45\": 2})\n",
    "df[\"race\"] = df[\"race\"].replace({\"Asian\": \"Minority\", \"Native American\": \"Minority\", \"Other\": \"Minority\"})\n",
    "\n",
    "df[\"sensitive\"] = df.apply(lambda x: x[\"race\"][0] + x[\"sex\"][0], axis=1)\n",
    "\n",
    "df.head()\n",
    "df.to_csv(\"data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sensitive  c_charge_degree  two_year_recid  count   sum      prob\n",
      "0         AF                0               0    157   229  0.685590\n",
      "1         AF                0               1     72   229  0.314410\n",
      "2         AF                1               0    248   423  0.586288\n",
      "3         AF                1               1    175   423  0.413712\n",
      "4         AM                0               0    470   920  0.510870\n",
      "5         AM                0               1    450   920  0.489130\n",
      "6         AM                1               0    920  2124  0.433145\n",
      "7         AM                1               1   1204  2124  0.566855\n",
      "8         CF                0               0    190   259  0.733591\n",
      "9         CF                0               1     69   259  0.266409\n",
      "10        CF                1               0    178   308  0.577922\n",
      "11        CF                1               1    130   308  0.422078\n",
      "12        CM                0               0    459   715  0.641958\n",
      "13        CM                0               1    256   715  0.358042\n",
      "14        CM                1               0    661  1172  0.563993\n",
      "15        CM                1               1    511  1172  0.436007\n",
      "16        HF                0               0     38    50  0.760000\n",
      "17        HF                0               1     12    50  0.240000\n",
      "18        HF                1               0     32    53  0.603774\n",
      "19        HF                1               1     21    53  0.396226\n",
      "20        HM                0               0    151   218  0.692661\n",
      "21        HM                0               1     67   218  0.307339\n",
      "22        HM                1               0    184   316  0.582278\n",
      "23        HM                1               1    132   316  0.417722\n",
      "24        MF                0               0     22    28  0.785714\n",
      "25        MF                0               1      6    28  0.214286\n",
      "26        MF                1               0     32    45  0.711111\n",
      "27        MF                1               1     13    45  0.288889\n",
      "28        MM                0               0     92   129  0.713178\n",
      "29        MM                0               1     37   129  0.286822\n",
      "30        MM                1               0    129   225  0.573333\n",
      "31        MM                1               1     96   225  0.426667\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_clean.csv\")\n",
    "\n",
    "df_count = df.groupby([\"sensitive\", \"c_charge_degree\", \"two_year_recid\"]).agg(\n",
    "    count=('race', 'count')).reset_index()\n",
    "\n",
    "df_sum = df_count.groupby([\"sensitive\", \"c_charge_degree\"]).agg(\n",
    "    sum=('count', 'sum')).reset_index()\n",
    "\n",
    "df_summary = pd.merge(df_count, df_sum, on=[\"sensitive\", \"c_charge_degree\"])\n",
    "\n",
    "df_summary[\"prob\"] = df_summary[\"count\"] / df_summary[\"sum\"]\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group fairness\n",
    "\n",
    "only sensitive data as prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sensitive  two_year_recid  count   sum      prob\n",
      "0         AF               0    405   652  0.621166\n",
      "1         AF               1    247   652  0.378834\n",
      "2         AM               0   1390  3044  0.456636\n",
      "3         AM               1   1654  3044  0.543364\n",
      "4         CF               0    368   567  0.649030\n",
      "5         CF               1    199   567  0.350970\n",
      "6         CM               0   1120  1887  0.593535\n",
      "7         CM               1    767  1887  0.406465\n",
      "8         HF               0     70   103  0.679612\n",
      "9         HF               1     33   103  0.320388\n",
      "10        HM               0    335   534  0.627341\n",
      "11        HM               1    199   534  0.372659\n",
      "12        MF               0     54    73  0.739726\n",
      "13        MF               1     19    73  0.260274\n",
      "14        MM               0    221   354  0.624294\n",
      "15        MM               1    133   354  0.375706\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_clean.csv\")\n",
    "\n",
    "df_count = df.groupby([\"sensitive\", \"two_year_recid\"]).agg(\n",
    "    count=('race', 'count')).reset_index()\n",
    "\n",
    "df_sum = df_count.groupby([\"sensitive\"]).agg(\n",
    "    sum=('count', 'sum')).reset_index()\n",
    "\n",
    "df_summary = pd.merge(df_count, df_sum, on=[\"sensitive\"])\n",
    "\n",
    "df_summary[\"prob\"] = df_summary[\"count\"] / df_summary[\"sum\"]\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.81      0.72      1207\n",
      "           1       0.64      0.44      0.52       958\n",
      "\n",
      "    accuracy                           0.64      2165\n",
      "   macro avg       0.64      0.62      0.62      2165\n",
      "weighted avg       0.64      0.64      0.63      2165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_clean.csv\")\n",
    "\n",
    "# Select relevant columns for features and target\n",
    "features = df.iloc[:, 3:-2]\n",
    "\n",
    "target = df['two_year_recid']\n",
    "\n",
    "# Handle missing values if any\n",
    "features = features.fillna(0)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = X_test.copy()\n",
    "df_test['two_year_recid_actual'] = y_test\n",
    "df_test['two_year_recid_predicted'] = y_pred\n",
    "\n",
    "df_test[\"sensitive\"] = df.loc[df_test.index.values, \"sensitive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sensitive  c_charge_degree  two_year_recid_predicted  count  sum      prob\n",
      "0         AF                0                         0     59   65  0.907692\n",
      "1         AF                0                         1      6   65  0.092308\n",
      "2         AF                1                         0     73  109  0.669725\n",
      "3         AF                1                         1     36  109  0.330275\n",
      "4         AM                0                         0    213  272  0.783088\n",
      "5         AM                0                         1     59  272  0.216912\n",
      "6         AM                1                         0    331  666  0.496997\n",
      "7         AM                1                         1    335  666  0.503003\n",
      "8         CF                0                         0     73   78  0.935897\n",
      "9         CF                0                         1      5   78  0.064103\n",
      "10        CF                1                         0     61   87  0.701149\n",
      "11        CF                1                         1     26   87  0.298851\n",
      "12        CM                0                         0    200  223  0.896861\n",
      "13        CM                0                         1     23  223  0.103139\n",
      "14        CM                1                         0    256  352  0.727273\n",
      "15        CM                1                         1     96  352  0.272727\n",
      "16        HF                0                         0     12   13  0.923077\n",
      "17        HF                0                         1      1   13  0.076923\n",
      "18        HF                1                         0     17   21  0.809524\n",
      "19        HF                1                         1      4   21  0.190476\n",
      "20        HM                0                         0     53   57  0.929825\n",
      "21        HM                0                         1      4   57  0.070175\n",
      "22        HM                1                         0     61   92  0.663043\n",
      "23        HM                1                         1     31   92  0.336957\n",
      "24        MF                0                         0     10   10  1.000000\n",
      "25        MF                1                         0     12   14  0.857143\n",
      "26        MF                1                         1      2   14  0.142857\n",
      "27        MM                0                         0     33   35  0.942857\n",
      "28        MM                0                         1      2   35  0.057143\n",
      "29        MM                1                         0     51   71  0.718310\n",
      "30        MM                1                         1     20   71  0.281690\n"
     ]
    }
   ],
   "source": [
    "# twin test for model\n",
    "\n",
    "df_count = df_test.groupby([\"sensitive\", \"c_charge_degree\", \"two_year_recid_predicted\"]).agg(\n",
    "    count=('sensitive', 'count')).reset_index()\n",
    "\n",
    "df_sum = df_count.groupby([\"sensitive\", \"c_charge_degree\"]).agg(\n",
    "    sum=('count', 'sum')).reset_index()\n",
    "\n",
    "df_summary = pd.merge(df_count, df_sum, on=[\"sensitive\", \"c_charge_degree\"])\n",
    "\n",
    "df_summary[\"prob\"] = df_summary[\"count\"] / df_summary[\"sum\"]\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note! When looking at c_charge_degree we found that the model was more biased toward African-American males than Caucasian males (prediction for AM 0.5 vs. CM 0.27; in data AM 0.56 and CM 0.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensitive\n",
       "AM    351\n",
       "CM    203\n",
       "CF     65\n",
       "AF     55\n",
       "HM     49\n",
       "MM     32\n",
       "HF     11\n",
       "MF      8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test[df_test[\"two_year_recid_actual\"] != df_test['two_year_recid_predicted']]\n",
    "\n",
    "# samples misclassified, count by sensitive information\n",
    "df_test[\"sensitive\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
